{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a55a584b-911f-4409-96a1-ef3ff689ec7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys, os\n",
    "import json\n",
    "import torch.nn as nn  \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import prettytable\n",
    "import time\n",
    "from thop.profile import profile\n",
    "\n",
    "from PIL import Image\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torchsummary import summary\n",
    "from tqdm.notebook import tqdm\n",
    "import seaborn as sns\n",
    "\n",
    "from utils import ImageShow,draw_size_acc,one_hot\n",
    "from utils import confusion_matrix,metrics_scores,pff\n",
    "\n",
    "from model import make_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "671c36f3-3fc8-4224-b165-903ca752a50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings.\n",
    "sys.path.append(os.pardir)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else \"cpu\")\n",
    "img_title = \"HAM10000\"\n",
    "best_acc = 0.\n",
    "eval_acc = 0.\n",
    "best_train = 0.\n",
    "dict_batch = {}\n",
    "dict_imgSize = {}\n",
    "\n",
    "#defined \n",
    "try:\n",
    "    print(len(train_acc_list))\n",
    "except NameError:\n",
    "    train_loss_list = []\n",
    "    train_acc_list = []\n",
    "    test_loss_list = []\n",
    "    test_acc_list = []\n",
    "    test_auc_list = []\n",
    "    val_loss_list = []\n",
    "    val_acc_list = []\n",
    "#activate ImageShow\n",
    "show = ImageShow(train_loss_list = train_loss_list,\n",
    "                 train_acc_list = train_acc_list,\n",
    "                test_loss_list = test_loss_list,\n",
    "                test_acc_list = test_acc_list,\n",
    "                test_auc_list = test_auc_list,\n",
    "                val_loss_list = val_loss_list,\n",
    "                val_acc_list = val_acc_list,\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee210751-649e-4c4a-822f-3d2d136e3313",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(trans_test='312'):\n",
    "    global test_dataset,train_loader,val_loader,test_loader\n",
    "    global train_num,val_num,test_num,n_classes,cla_dict\n",
    "    data_transform = {\n",
    "        \"train\": transforms.Compose([transforms.RandomResizedCrop((299, 299)),\n",
    "                                     transforms.RandomVerticalFlip(),\n",
    "                                     transforms.ToTensor(),\n",
    "                                     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]),\n",
    "        \"val\": transforms.Compose([transforms.Resize((302,302)),\n",
    "                                   transforms.CenterCrop((299, 299)),\n",
    "                                   transforms.ToTensor(),\n",
    "                                   transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "                                  ]),\n",
    "        \"test\": transforms.Compose([transforms.Resize((trans_test,trans_test)),\n",
    "                                   transforms.CenterCrop((299, 299)),\n",
    "                                   transforms.ToTensor(),\n",
    "                                   transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "                                  ])\n",
    "        }\n",
    "\n",
    "    data_root = os.path.abspath(os.path.join(os.getcwd(),\"..\"))  # get data root path\n",
    "    image_path = os.path.join(data_root)\n",
    "    assert os.path.exists(image_path), \"{} path does not exist.\".format(image_path)\n",
    "\n",
    "    train_dataset = datasets.ImageFolder(root=os.path.join(image_path,train_doc),# train_doc=aug_train_8000\n",
    "                                         transform=data_transform[\"train\"])\n",
    "    val_dataset = datasets.ImageFolder(root=os.path.join(image_path,val_doc),# val_doc=val_dir\n",
    "                                            transform=data_transform[\"val\"])\n",
    "    test_dataset = datasets.ImageFolder(root=os.path.join(image_path,test_doc),# test_doc=test_dir\n",
    "                                            transform=data_transform[\"test\"])\n",
    "\n",
    "    train_num = len(train_dataset)\n",
    "    val_num = len(val_dataset)\n",
    "    test_num = len(test_dataset)\n",
    "    \n",
    "    data_list = train_dataset.class_to_idx\n",
    "    cla_dict = dict((val, key) for key, val in data_list.items())\n",
    "    n_classes  = len(data_list)\n",
    "    print(f'Using {n_classes } classes.')\n",
    "    # write dict into json file\n",
    "    json_str = json.dumps(cla_dict, indent=4)\n",
    "    with open(f'{img_title}.json', 'w') as json_file:#class_indices\n",
    "        json_file.write(json_str)\n",
    "        \n",
    "    pin_memory = True\n",
    "    train_loader = DataLoader(train_dataset,batch_size=BatchSize,\n",
    "                                               pin_memory=pin_memory,\n",
    "                                               shuffle=True,num_workers=nw)\n",
    "    val_loader = DataLoader(val_dataset,batch_size=V_size,\n",
    "                                               pin_memory=pin_memory,\n",
    "                                               shuffle=False,num_workers=nw)\n",
    "    test_loader = DataLoader(test_dataset,batch_size=T_size,\n",
    "                                              pin_memory=pin_memory,\n",
    "                                              shuffle=False,num_workers=nw)\n",
    "\n",
    "    print(\"using {} images for training, {} images for validation, {} images for testing.\".format(train_num,\n",
    "                                                                                                  val_num,\n",
    "                                                                                                  test_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f648fb55-02a4-4a99-98a6-11aa83216691",
   "metadata": {},
   "outputs": [],
   "source": [
    "BatchSize = 168\n",
    "V_size = 40 \n",
    "T_size = 32 \n",
    "train_doc = \"aug_train_8000\"\n",
    "val_doc = \"val_dir\"\n",
    "test_doc = \"test_dir\"\n",
    "\n",
    "nw = min([os.cpu_count(), BatchSize if BatchSize > 1 else 0, 6]) \n",
    "print(f'Using {nw} dataloader workers every process.')\n",
    "get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "39b16539-a4b9-45bf-9a7d-5b73a7909ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module): \n",
    "    def __init__(self, n_channels=3, img_size=299, num_classes=7, conv_layers_num=2): # original input size 299*299*3\n",
    "        super().__init__()\n",
    "        \n",
    "        self.sz = cal_size()\n",
    "        output_ch = [16, 32]\n",
    "        fc_ch = [256]\n",
    "        \n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(n_channels, output_ch[0], kernel_size=3, stride=1, padding=1),  # First convolution feature map: 299*299\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # Downsampling feature map: 149*149\n",
    "\n",
    "            nn.Conv2d(output_ch[0], output_ch[1], kernel_size=3, stride=1, padding=1),  # Second convolution feature map: 149*149\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # Downsampling feature map: 74*74 (sz=74)\n",
    "        )\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(output_ch[1] * self.sz * self.sz, fc_ch[0]), \n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(fc_ch[0], num_classes),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = self.fc_layers(x)\n",
    "        return x\n",
    "\n",
    "    def cal_size(self):\n",
    "        tmp = img_size\n",
    "        for i in range(conv_layers_num):\n",
    "            tmp = int(np.floor((tmp-2)/2)+1)\n",
    "        return tmp\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8b2bf7e8-01e0-4941-a212-85a49673c5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module): \n",
    "    def __init__(self, n_channels=3, img_size=299, num_classes=7, conv_layers_num=2): # original input size 299*299*3\n",
    "        super().__init__()\n",
    "        \n",
    "        self.sz = cal_size()\n",
    "        output_ch = [16, 32]\n",
    "        \n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(n_channels, output_ch[0], kernel_size=3, stride=1, padding=1),  # First convolution feature map: 299*299\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # Downsampling feature map: 149*149\n",
    "\n",
    "            nn.Conv2d(output_ch[0], output_ch[1], kernel_size=3, stride=1, padding=1),  # Second convolution feature map: 149*149\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # Downsampling feature map: 74*74 (sz=74)\n",
    "        )\n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(output_ch[1], num_classes),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = self.global_avg_pool(x)\n",
    "        x = self.fc_layers(x)\n",
    "        return x\n",
    "\n",
    "    def cal_size(self):\n",
    "        tmp = img_size\n",
    "        for i in range(conv_layers_num):\n",
    "            tmp = int(np.floor((tmp-2)/2)+1)\n",
    "        return tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ca41b2e9-782e-4832-aa62-02b5aada1542",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 16, 299, 299]             448\n",
      "              ReLU-2         [-1, 16, 299, 299]               0\n",
      "         MaxPool2d-3         [-1, 16, 149, 149]               0\n",
      "            Conv2d-4         [-1, 32, 149, 149]           4,640\n",
      "              ReLU-5         [-1, 32, 149, 149]               0\n",
      "         MaxPool2d-6           [-1, 32, 74, 74]               0\n",
      " AdaptiveAvgPool2d-7             [-1, 32, 1, 1]               0\n",
      "           Flatten-8                   [-1, 32]               0\n",
      "            Linear-9                    [-1, 7]             231\n",
      "================================================================\n",
      "Total params: 5,319\n",
      "Trainable params: 5,319\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 1.02\n",
      "Forward/backward pass size (MB): 36.71\n",
      "Params size (MB): 0.02\n",
      "Estimated Total Size (MB): 37.76\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "n_channels = 3\n",
    "img_size = 299\n",
    "network = SimpleCNN(n_channels=n_channels, img_size=img_size, num_classes=7, conv_layers_num=2)\n",
    "network = network.to(device)\n",
    "summary(network, (n_channels,img_size,img_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9c5ea661-78f2-47dc-9b76-706d923e2be5",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m dsize \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m299\u001b[39m, \u001b[38;5;241m299\u001b[39m)\n\u001b[0;32m      2\u001b[0m input_data \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(dsize)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m----> 3\u001b[0m \u001b[43mpff\u001b[49m\u001b[43m(\u001b[49m\u001b[43mm_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSimpleCNN\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnetwork\u001b[49m\u001b[43m,\u001b[49m\u001b[43minputes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_data\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\221Project\\HAM10K\\utils.py:203\u001b[0m, in \u001b[0;36mpff\u001b[1;34m(m_name, model, inputes)\u001b[0m\n\u001b[0;32m    201\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 203\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msynchronize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    204\u001b[0m     start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m    205\u001b[0m     output\u001b[38;5;241m=\u001b[39m model(inputes)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\DL_image\\lib\\site-packages\\torch\\cuda\\__init__.py:799\u001b[0m, in \u001b[0;36msynchronize\u001b[1;34m(device)\u001b[0m\n\u001b[0;32m    791\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msynchronize\u001b[39m(device: _device_t \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    792\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Wait for all kernels in all streams on a CUDA device to complete.\u001b[39;00m\n\u001b[0;32m    793\u001b[0m \n\u001b[0;32m    794\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    797\u001b[0m \u001b[38;5;124;03m            if :attr:`device` is ``None`` (default).\u001b[39;00m\n\u001b[0;32m    798\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 799\u001b[0m     \u001b[43m_lazy_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    800\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdevice(device):\n\u001b[0;32m    801\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_cuda_synchronize()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\DL_image\\lib\\site-packages\\torch\\cuda\\__init__.py:293\u001b[0m, in \u001b[0;36m_lazy_init\u001b[1;34m()\u001b[0m\n\u001b[0;32m    288\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    289\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    290\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiprocessing, you must use the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspawn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m start method\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    291\u001b[0m     )\n\u001b[0;32m    292\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_cuda_getDeviceCount\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 293\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTorch not compiled with CUDA enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    294\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _cudart \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    295\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[0;32m    296\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    297\u001b[0m     )\n",
      "\u001b[1;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "# The following cannot run on CPUs\n",
    "dsize = (1, 3, 299, 299)\n",
    "input_data = torch.randn(dsize).to(device)\n",
    "pff(m_name=\"SimpleCNN\",model=network,inputes=input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7c432361-7680-4574-9cf6-8c1501e1d52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    network.train()\n",
    "    global best_train,train_evl_result#,evl_tmp_result\n",
    "    running_loss,r_pre = 0., 0.\n",
    "    print_step = len(train_loader)//2\n",
    "    steps_num = len(train_loader)\n",
    "    tmp_size = BatchSize\n",
    "    print(f'\\033[1;32m[Train Epoch:[{epoch}]{img_title} ==> Training]\\033[0m ...')\n",
    "    optimizer.zero_grad()\n",
    "    train_tmp_result = torch.zeros(n_classes,n_classes)\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(tqdm(train_loader)):        \n",
    "\n",
    "        batch_idx += 1\n",
    "        target_indices = target\n",
    "        target_one_hot = one_hot(target, length=n_classes)\n",
    "        data, target = Variable(data).to(device), Variable(target_one_hot).to(device)\n",
    "\n",
    "        output = network(data)\n",
    "        loss = network.loss(output, target, size_average=True)       \n",
    "        loss.backward()     \n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        v_mag = torch.sqrt(torch.sum(output**2, dim=2, keepdim=True)) \n",
    "        pred = v_mag.data.max(1, keepdim=True)[1].cpu().squeeze()\n",
    "        r_pre += pred.eq(target_indices.view_as(pred)).squeeze().sum()\n",
    "        tmp_pre = r_pre/(batch_idx*BatchSize)\n",
    "        \n",
    "        if batch_idx % print_step == 0 and batch_idx != steps_num:\n",
    "            print(\"[{}/{}] Loss{:.5f},ACC:{:.5f}\".format(batch_idx,len(train_loader), # steps_num = len(train_loader)\n",
    "                                                         loss,tmp_pre))\n",
    "        if batch_idx % steps_num == 0 and train_num % tmp_size != 0:  # train_num = len(train_dataset), tmp_size = BatchSize\n",
    "            tmp_size = train_num % tmp_size\n",
    "                          \n",
    "        for i in range(tmp_size):\n",
    "            pred_x = pred.numpy()\n",
    "            train_tmp_result[target_indices[i]][pred_x[i]] +=1\n",
    "\n",
    "        if best_train < tmp_pre and tmp_pre >= 80: \n",
    "            torch.save(network.state_dict(), iter_path)\n",
    "        \n",
    "    epoch_acc = r_pre / train_num\n",
    "    epoch_loss = running_loss / len(train_loader)  \n",
    "    train_loss_list.append(epoch_loss)\n",
    "    train_acc_list.append(epoch_acc) \n",
    "    scheduler.step()\n",
    "    if best_train < epoch_acc:\n",
    "        best_train = epoch_acc\n",
    "        train_evl_result = train_tmp_result.clone()\n",
    "        torch.save(network.state_dict(), last_path)\n",
    "        torch.save(train_evl_result, f'./tmp/{img_title}/{suf}/train_evl_result.pth')\n",
    "    \n",
    "    print(\"Train Epoch:[{}] Loss:{:.5f},Acc:{:.5f},Best_train:{:.5f}\".format(epoch,epoch_loss,\n",
    "                                                                     epoch_acc,best_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe72d23-b7f1-4a56-90ef-1c5cb3158994",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(split=\"test\"):\n",
    "    network.eval()\n",
    "    global test_acc,eval_acc,best_acc,net_parameters\n",
    "    global test_evl_result,val_evl_result#,evl_tmp_result\n",
    "    cor_loss,correct,Auc, Acc= 0, 0, 0, 0\n",
    "    evl_tmp_result = torch.zeros(n_classes,n_classes)\n",
    "    \n",
    "    if split == 'val':\n",
    "        data_loader = val_loader\n",
    "        tmp_size = V_size\n",
    "        data_num = val_num\n",
    "    else:\n",
    "        data_loader = test_loader\n",
    "        tmp_size = T_size\n",
    "        data_num = test_num\n",
    "        \n",
    "    steps_num = len(data_loader)\n",
    "    print(f'\\033[35m{img_title} ==> {split} ...\\033[0m')\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, target) in enumerate(tqdm(data_loader)):\n",
    "            batch_idx +=1\n",
    "            target_indices = target#torch.Size([batch, 7])  \n",
    "            target_one_hot = one_hot(target, length=n_classes)            \n",
    "            data, target = Variable(data).to(device), Variable(target_one_hot).to(device)\n",
    "\n",
    "            output= network(data)#torch.Size([batch_size, 7, 16, 1])         \n",
    "            v_mag = torch.sqrt(torch.sum(output**2, dim=2, keepdim=True))\n",
    "            pred = v_mag.data.max(1, keepdim=True)[1].cpu()#[9, 2, 1, 1, 6,..., 1, 4, 6, 5, 7,]\n",
    "            \n",
    "            if batch_idx % steps_num == 0 and data_num % tmp_size != 0:\n",
    "                tmp_size = data_num % tmp_size\n",
    "                          \n",
    "            for i in range(tmp_size):\n",
    "                pred_y = pred.numpy()\n",
    "                evl_tmp_result[target_indices[i]][pred_y[i]] +=1 \n",
    "\n",
    "        diag_sum = torch.sum(evl_tmp_result.diagonal())\n",
    "        all_sum = torch.sum(evl_tmp_result) \n",
    "        test_acc = 100. * float(torch.div(diag_sum,all_sum)) \n",
    "        print(f\"{split}_Acc:\\033[1;32m{round(float(test_acc),3)}%\\033[0m\")\n",
    "\n",
    "        if split == 'val':\n",
    "            val_acc_list.append(test_acc)\n",
    "            if test_acc >= best_acc:\n",
    "                best_acc = test_acc\n",
    "                val_evl_result = evl_tmp_result.clone()#copy.deepcopy(input)\n",
    "                torch.save(network.state_dict(), save_PATH)\n",
    "                torch.save(val_evl_result, f'./tmp/{img_title}/{suf}/best_evl_result.pth')\n",
    "            print(f\"Best_val:\\033[1;32m[{round(float(best_acc),3)}%]\\033[0m\")\n",
    "        else:\n",
    "            test_acc_list.append(test_acc)\n",
    "            if test_acc >= eval_acc:\n",
    "                eval_acc = test_acc\n",
    "                test_evl_result = evl_tmp_result.clone()#copy.deepcopy(input)\n",
    "                torch.save(network.state_dict(), f'./tmp/{img_title}/{suf}/{split}_best_{img_title}_{suf}.pth')\n",
    "                torch.save(test_evl_result, f'./tmp/{img_title}/{suf}/{split}_evl_result.pth')\n",
    "            print(f\"Best_eval:\\033[1;32m[{round(float(eval_acc),3)}%]\\033[0m\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215cfc87-80d7-4046-be3d-9174dd588704",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf9f10c-2bb4-4926-b096-bcc1d8602326",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
